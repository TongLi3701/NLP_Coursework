{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVR+Sentence_embed_Bert_multilingual+raw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWVNrSepufI",
        "colab_type": "text"
      },
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m-5YiHHftBX",
        "colab_type": "text"
      },
      "source": [
        "## Dowdload and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNdY9X-fSlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A0Z6AWU37Gg",
        "colab_type": "text"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w8HNiip39OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path):\n",
        "    \"\"\"\n",
        "    Read data from the data path.\n",
        "\n",
        "    Args: \n",
        "        path: the path of the dataset, normally in local folder.\n",
        "    \n",
        "    Returns:\n",
        "        Loaded raw dataset. \n",
        "    \"\"\"\n",
        "    with open(path) as dataset:\n",
        "        raw_data = dataset.readlines()\n",
        "\n",
        "    return raw_data\n",
        "\n",
        "\n",
        "# Define the path of the train dataset\n",
        "english_train_path = \"train.enzh.src\"\n",
        "chinese_train_path = \"train.enzh.mt\"   \n",
        "scores_train_path = \"train.enzh.scores\"\n",
        "# Define the path of the validatin dataset\n",
        "english_validation_path = \"dev.enzh.src\"\n",
        "chinese_validation_path = \"dev.enzh.mt\"   \n",
        "scores_validation_path = \"dev.enzh.scores\"\n",
        "# Define the path of the test dataset\n",
        "english_test_path = \"test.enzh.src\"\n",
        "chinese_test_path = \"test.enzh.mt\"\n",
        "\n",
        "\n",
        "# Read train, validation, test data\n",
        "raw_english_train = read_data(english_train_path)\n",
        "raw_chinese_train = read_data(chinese_train_path)\n",
        "raw_english_validation = read_data(english_validation_path)\n",
        "raw_chinese_validation = read_data(chinese_validation_path)\n",
        "raw_english_test = read_data(english_test_path)\n",
        "raw_chinese_test = read_data(chinese_test_path)\n",
        "\n",
        "# read scores for train and validation dataset \n",
        "score_train = read_data(scores_train_path)\n",
        "score_validation = read_data(scores_validation_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z99k58qJmwV",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Embedding - BERT - Multilingual models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4s48XidZa2r",
        "colab_type": "text"
      },
      "source": [
        "Download and Import:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXFF_aTdXT9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-serving-server # server\n",
        "!pip install bert-serving-client # client, independent of 'bert-serving-server'\n",
        "from bert_serving.client import BertClient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOlpbI8jec1x",
        "colab_type": "text"
      },
      "source": [
        "#### Multilingual model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzyBmz5Lcgvm",
        "colab_type": "text"
      },
      "source": [
        "Base model:\n",
        "\n",
        "(12-layer, 768-hidden, 12-heads, 110M parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHDaZHKSl5GA",
        "colab_type": "code",
        "outputId": "c4443ee0-20a2-4856-ea77-fd53db070138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# download pretrained BERT model cased_L-12_H-768_A-12\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip multi_cased_L-12_H-768_A-12.zip\n",
        "ml_model = \"multi_cased_L-12_H-768_A-12\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 15:08:02--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 2404:6800:4008:c03::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M   145MB/s    in 4.2s    \n",
            "\n",
            "2020-02-28 15:08:06 (151 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n",
            "Archive:  multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84-B1qTePO3T",
        "colab_type": "text"
      },
      "source": [
        "Obtain English results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1XdIUpXPOd4",
        "colab_type": "code",
        "outputId": "ad28b1b9-70eb-4a72-df81-1d590a74b0d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# start server\n",
        "!nohup bert-serving-start -model_dir=./{ml_model} > out.file 2>&1 &\n",
        "# open client\n",
        "bc_ml = BertClient()\n",
        "# encode English raw corpus\n",
        "english_train_embeddings = bc_ml.encode(raw_english_train)\n",
        "english_val_embeddings = bc_ml.encode(raw_english_validation)\n",
        "english_test_embeddings = bc_ml.encode(raw_english_test)\n",
        "chninese_train_embeddings = bc_ml.encode(raw_chinese_train)\n",
        "chinese_val_embeddings = bc_ml.encode(raw_chinese_validation)\n",
        "chinese_test_embeddings = bc_ml.encode(raw_chinese_test)\n",
        "# close client\n",
        "bc_ml.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bert_serving/client/__init__.py:299: UserWarning: some of your sentences have more tokens than \"max_seq_len=25\" set on the server, as consequence you may get less-accurate or truncated embeddings.\n",
            "here is what you can do:\n",
            "- disable the length-check by create a new \"BertClient(check_length=False)\" when you do not want to display this warning\n",
            "- or, start a new server with a larger \"max_seq_len\"\n",
            "  '- or, start a new server with a larger \"max_seq_len\"' % self.length_limit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z0rLZSUZLjA",
        "colab_type": "text"
      },
      "source": [
        "### Concatenate vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLS7vjqXd_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate vectors\n",
        "sentence_embeddings_train = []\n",
        "sentence_embeddings_val = []    \n",
        "sentence_embeddings_test = []\n",
        "\n",
        "# Concatenate train vectors\n",
        "for i in range(len(english_train_embeddings)):\n",
        "    english = list(english_train_embeddings[i])\n",
        "    chinese = list(chninese_train_embeddings[i])\n",
        "    english.extend(chinese)\n",
        "    sentence_embeddings_train.append(english)\n",
        "\n",
        "# Concatenate validation vectors\n",
        "for i in range(len(english_val_embeddings)):\n",
        "    english = list(english_val_embeddings[i])\n",
        "    chinese = list(chinese_val_embeddings[i])\n",
        "    english.extend(chinese)\n",
        "    sentence_embeddings_val.append(english)\n",
        "    \n",
        "# Concatenate test vectors\n",
        "for i in range(len(english_test_embeddings)):\n",
        "    english = list(english_test_embeddings[i])\n",
        "    chinese = list(chinese_test_embeddings[i])\n",
        "    english.extend(chinese)\n",
        "    sentence_embeddings_test.append(english)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRe8WNXyYoDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_embeddings_train = np.asarray(sentence_embeddings_train).astype(float)\n",
        "sentence_embeddings_val = np.asarray(sentence_embeddings_val).astype(float)\n",
        "sentence_embeddings_test = np.asarray(sentence_embeddings_test).astype(float)\n",
        "\n",
        "score_train = np.asarray(score_train).astype(float)\n",
        "score_validation = np.asarray(score_validation).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxB7hNT7Fh5X",
        "colab_type": "text"
      },
      "source": [
        "## Model: SVR\n",
        "\n",
        "It achieves the best result when k is set as 'rbf' based on experiment (in the commented part)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8uqnvCJoimG",
        "colab_type": "code",
        "outputId": "36303aee-d74a-49e9-895a-6b479bf02006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    \"\"\"\n",
        "    Method to calculate the root mean squared error.\n",
        "\n",
        "    Args: \n",
        "        predictions: the prediction of the model.\n",
        "        targets: the ground truth.\n",
        "    \n",
        "    Returns:\n",
        "        The sentence vector.\n",
        "    \"\"\"\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "# Train and fit into the model.\n",
        "clf_t = SVR(kernel='rbf')\n",
        "clf_t.fit(sentence_embeddings_train, score_train)\n",
        "predictions = clf_t.predict(sentence_embeddings_val)\n",
        "pearson = pearsonr(score_validation, predictions)\n",
        "print(f'RMSE: {rmse(predictions,score_validation)} Pearson {pearson[0]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.8879345425076625 Pearson 0.35773168904694735\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}