{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNN+Sentence_embed_sentence_transformer+raw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWVNrSepufI",
        "colab_type": "text"
      },
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m-5YiHHftBX",
        "colab_type": "text"
      },
      "source": [
        "## Dowdload and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNdY9X-fSlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A0Z6AWU37Gg",
        "colab_type": "text"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w8HNiip39OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path):\n",
        "    \"\"\"\n",
        "    Read data from the data path.\n",
        "\n",
        "    Args: \n",
        "        path: the path of the dataset, normally in local folder.\n",
        "    \n",
        "    Returns:\n",
        "        Loaded raw dataset. \n",
        "    \"\"\"\n",
        "    with open(path) as dataset:\n",
        "        raw_data = dataset.readlines()\n",
        "\n",
        "    return raw_data\n",
        "\n",
        "\n",
        "# Define the path of the train dataset\n",
        "english_train_path = \"train.enzh.src\"\n",
        "chinese_train_path = \"train.enzh.mt\"   \n",
        "scores_train_path = \"train.enzh.scores\"\n",
        "# Define the path of the validatin dataset\n",
        "english_validation_path = \"dev.enzh.src\"\n",
        "chinese_validation_path = \"dev.enzh.mt\"   \n",
        "scores_validation_path = \"dev.enzh.scores\"\n",
        "# Define the path of the test dataset\n",
        "english_test_path = \"test.enzh.src\"\n",
        "chinese_test_path = \"test.enzh.mt\"\n",
        "\n",
        "\n",
        "# Read train, validation, test data\n",
        "raw_english_train = read_data(english_train_path)\n",
        "raw_chinese_train = read_data(chinese_train_path)\n",
        "raw_english_validation = read_data(english_validation_path)\n",
        "raw_chinese_validation = read_data(chinese_validation_path)\n",
        "raw_english_test = read_data(english_test_path)\n",
        "raw_chinese_test = read_data(chinese_test_path)\n",
        "\n",
        "# read scores for train and validation dataset \n",
        "score_train = read_data(scores_train_path)\n",
        "score_validation = read_data(scores_validation_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej1vYxsTFdH3",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Embedding - SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rGZoOfqXDQ5",
        "colab_type": "code",
        "outputId": "2799fa66-592a-439d-cc4b-6d11c2cd419b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.7MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 71.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 61.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.11.15)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.14.15)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (2.6.1)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5-cp36-none-any.whl size=64942 sha256=6e1f4bb0d4e9c44143265bdcd50cb81551751b0cfc63b73858fd3278a3e9b65c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/ce/39/5bbda8ac34eb52df8c6531382ca077773fbfcbfb6386e5d66c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=31a07e545d71485f060a6626c9f36f0e0f82a4def893d61f3856580a1275239a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7vteCDNet8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "fd0f1b82-af9d-4a0d-b50c-d8431002a7d6"
      },
      "source": [
        "# Import sentence transformer for sentence embedding.\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 504M/504M [10:44<00:00, 782kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-UqnllRiC-6",
        "colab_type": "text"
      },
      "source": [
        "Using raw corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE0Ijz9Oy80o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert preprocessed corpus to sentence embeddings\n",
        "english_train_embeddings = model.encode(raw_english_train)\n",
        "chninese_train_embeddings = model.encode(raw_chinese_train)\n",
        "\n",
        "english_val_embeddings = model.encode(raw_english_validation)\n",
        "chinese_val_embeddings = model.encode(raw_chinese_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSr9IF8Ln8XN",
        "colab_type": "text"
      },
      "source": [
        "#### Concatenate vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y08tMiNDn9s4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate vectors\n",
        "sentence_embeddings_train = []\n",
        "sentence_embeddings_val = []    \n",
        "sentence_embeddings_test = []\n",
        "\n",
        "# Concatenate train vectors\n",
        "for i in range(len(english_train_embeddings)):\n",
        "    english = list(english_train_embeddings[i])\n",
        "    chinese = list(chninese_train_embeddings[i])\n",
        "    english.extend(chinese)\n",
        "    sentence_embeddings_train.append(english)\n",
        "\n",
        "# Concatenate validation vectors\n",
        "for i in range(len(english_val_embeddings)):\n",
        "    english = list(english_val_embeddings[i])\n",
        "    chinese = list(chinese_val_embeddings[i])\n",
        "    english.extend(chinese)\n",
        "    sentence_embeddings_val.append(english)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD8uw4-3n_00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_embeddings_train = np.asarray(sentence_embeddings_train).astype(float)\n",
        "sentence_embeddings_val = np.asarray(sentence_embeddings_val).astype(float)\n",
        "sentence_embeddings_test = np.asarray(sentence_embeddings_test).astype(float)\n",
        "\n",
        "score_train = np.asarray(score_train).astype(float)\n",
        "score_validation = np.asarray(score_validation).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N5TsX1KqW9r",
        "colab_type": "text"
      },
      "source": [
        "## Model: Feedforward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTOXyjiQGCuQ",
        "colab_type": "text"
      },
      "source": [
        "Uses Sentence Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw7c_ecwqbR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feed data into data loader\n",
        "batch_size = 32\n",
        "\n",
        "train_data = []\n",
        "val_data = []\n",
        "\n",
        "for i in range(len(sentence_embeddings_train)):\n",
        "    train_data.append([sentence_embeddings_train[i], score_train[i]])\n",
        "\n",
        "for i in range(len(sentence_embeddings_val)):\n",
        "    val_data.append([sentence_embeddings_val[i], score_validation[i]])\n",
        "\n",
        "loader_train = torch.utils.data.DataLoader(train_data, batch_size = batch_size)\n",
        "loader_val = torch.utils.data.DataLoader(val_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cckOMwHGqkzH",
        "colab_type": "code",
        "outputId": "07f59cf3-760e-4530-b429-dd1a2ee86571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 50\n",
        "\n",
        "print('using device:', device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fafZ9sEXrDxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully Connected Feedforward Neural Network.\n",
        "    4 linear hidden layers \n",
        "    1 output layer\n",
        "\n",
        "    Attributes:\n",
        "        fc1: layer 1\n",
        "        fc2: layer 2\n",
        "        fc3: layer 3\n",
        "        fc4: layer 4\n",
        "        fc5: output layer\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):  \n",
        "        super(FFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4s8bddWg5zH",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip5JArbgrJDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "def check_train_performance(loader, model):\n",
        "    \"\"\"\n",
        "    Method to check the performance of the train process.\n",
        "\n",
        "    Args: \n",
        "        loader: the dataloder which contains the train data \n",
        "        model: the model to be trained.\n",
        "    \n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    real_scores = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "\n",
        "            x = x.to(device=device, dtype=dtype)\n",
        "            y = y.to(device=device, dtype=float)\n",
        "\n",
        "            score = model(x)\n",
        "            \n",
        "\n",
        "            predictions.append(score.cpu().detach().numpy())\n",
        "            real_scores.append(y.cpu().detach().numpy())\n",
        "\n",
        "    predictions = np.asarray([i for item in predictions for i in item]).squeeze(1)\n",
        "    \n",
        "    pearson = pearsonr(score_train, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,score_train)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\n",
        "def train_part(model, optimizer, scheduler=None, epochs=1):\n",
        "    \"\"\"\n",
        "    Method to train the model.\n",
        "\n",
        "    Args: \n",
        "        model: the model to be trained.\n",
        "        optimizer: the optimizer used for optimisation.\n",
        "        scheduler: the scheduler used in this training process\n",
        "        epochs: the number of epochs , default is 1\n",
        "    \n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.float)\n",
        "\n",
        "\n",
        "            scores = model(x)\n",
        "\n",
        "            scores = scores.squeeze(1)\n",
        "\n",
        "            loss = F.mse_loss(scores, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "                print('Epoch: %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "                #check_accuracy(loader_val, model)\n",
        "                print()\n",
        "        check_train_performance(loader_train, model)\n",
        "        # Adjust the learning rate\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEzjwjnhrO1_",
        "colab_type": "code",
        "outputId": "b7d10eb9-e62e-49fc-d59f-3e59e58bc952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initialisation of the model and train.\n",
        "baseline_model = FFNN()\n",
        "print(baseline_model)\n",
        "optimizer = optim.Adam(baseline_model.parameters())\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 10)\n",
        "train_part(baseline_model, optimizer, scheduler, epochs=20)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FFNN(\n",
            "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Epoch: 0, Iteration 0, loss = 0.8997\n",
            "\n",
            "Epoch: 0, Iteration 50, loss = 0.9164\n",
            "\n",
            "Epoch: 0, Iteration 100, loss = 0.7175\n",
            "\n",
            "Epoch: 0, Iteration 150, loss = 0.7734\n",
            "\n",
            "Epoch: 0, Iteration 200, loss = 0.7546\n",
            "\n",
            "RMSE: 0.8531191006600798 Pearson 0.42970294767892236\n",
            "\n",
            "Epoch: 1, Iteration 0, loss = 0.8022\n",
            "\n",
            "Epoch: 1, Iteration 50, loss = 0.7442\n",
            "\n",
            "Epoch: 1, Iteration 100, loss = 0.5506\n",
            "\n",
            "Epoch: 1, Iteration 150, loss = 0.6270\n",
            "\n",
            "Epoch: 1, Iteration 200, loss = 0.8567\n",
            "\n",
            "RMSE: 0.7913576009851085 Pearson 0.5512048578689073\n",
            "\n",
            "Epoch: 2, Iteration 0, loss = 0.8291\n",
            "\n",
            "Epoch: 2, Iteration 50, loss = 0.7209\n",
            "\n",
            "Epoch: 2, Iteration 100, loss = 0.4152\n",
            "\n",
            "Epoch: 2, Iteration 150, loss = 0.4559\n",
            "\n",
            "Epoch: 2, Iteration 200, loss = 0.7154\n",
            "\n",
            "RMSE: 0.7018230059771594 Pearson 0.6690581302840164\n",
            "\n",
            "Epoch: 3, Iteration 0, loss = 0.6317\n",
            "\n",
            "Epoch: 3, Iteration 50, loss = 0.4931\n",
            "\n",
            "Epoch: 3, Iteration 100, loss = 0.3396\n",
            "\n",
            "Epoch: 3, Iteration 150, loss = 0.3490\n",
            "\n",
            "Epoch: 3, Iteration 200, loss = 0.5751\n",
            "\n",
            "RMSE: 0.5662348561021969 Pearson 0.796789707193719\n",
            "\n",
            "Epoch: 4, Iteration 0, loss = 0.6534\n",
            "\n",
            "Epoch: 4, Iteration 50, loss = 0.3379\n",
            "\n",
            "Epoch: 4, Iteration 100, loss = 0.1949\n",
            "\n",
            "Epoch: 4, Iteration 150, loss = 0.1492\n",
            "\n",
            "Epoch: 4, Iteration 200, loss = 0.3149\n",
            "\n",
            "RMSE: 0.4966522188244719 Pearson 0.8583387572864746\n",
            "\n",
            "Epoch: 5, Iteration 0, loss = 0.5912\n",
            "\n",
            "Epoch: 5, Iteration 50, loss = 0.2379\n",
            "\n",
            "Epoch: 5, Iteration 100, loss = 0.3031\n",
            "\n",
            "Epoch: 5, Iteration 150, loss = 0.1499\n",
            "\n",
            "Epoch: 5, Iteration 200, loss = 0.1876\n",
            "\n",
            "RMSE: 0.3966960553081966 Pearson 0.9112899981645747\n",
            "\n",
            "Epoch: 6, Iteration 0, loss = 0.2106\n",
            "\n",
            "Epoch: 6, Iteration 50, loss = 0.1585\n",
            "\n",
            "Epoch: 6, Iteration 100, loss = 0.1335\n",
            "\n",
            "Epoch: 6, Iteration 150, loss = 0.0926\n",
            "\n",
            "Epoch: 6, Iteration 200, loss = 0.2025\n",
            "\n",
            "RMSE: 0.3517641545903849 Pearson 0.931704502694161\n",
            "\n",
            "Epoch: 7, Iteration 0, loss = 0.2028\n",
            "\n",
            "Epoch: 7, Iteration 50, loss = 0.0985\n",
            "\n",
            "Epoch: 7, Iteration 100, loss = 0.0863\n",
            "\n",
            "Epoch: 7, Iteration 150, loss = 0.1078\n",
            "\n",
            "Epoch: 7, Iteration 200, loss = 0.1553\n",
            "\n",
            "RMSE: 0.4144954168980412 Pearson 0.9381638198018026\n",
            "\n",
            "Epoch: 8, Iteration 0, loss = 0.1881\n",
            "\n",
            "Epoch: 8, Iteration 50, loss = 0.0620\n",
            "\n",
            "Epoch: 8, Iteration 100, loss = 0.0384\n",
            "\n",
            "Epoch: 8, Iteration 150, loss = 0.0813\n",
            "\n",
            "Epoch: 8, Iteration 200, loss = 0.1559\n",
            "\n",
            "RMSE: 0.31687638589572503 Pearson 0.9512214732762952\n",
            "\n",
            "Epoch: 9, Iteration 0, loss = 0.1740\n",
            "\n",
            "Epoch: 9, Iteration 50, loss = 0.1136\n",
            "\n",
            "Epoch: 9, Iteration 100, loss = 0.0533\n",
            "\n",
            "Epoch: 9, Iteration 150, loss = 0.0315\n",
            "\n",
            "Epoch: 9, Iteration 200, loss = 0.0961\n",
            "\n",
            "RMSE: 0.33662620925410613 Pearson 0.95284000893633\n",
            "\n",
            "Epoch: 10, Iteration 0, loss = 0.1354\n",
            "\n",
            "Epoch: 10, Iteration 50, loss = 0.0353\n",
            "\n",
            "Epoch: 10, Iteration 100, loss = 0.0657\n",
            "\n",
            "Epoch: 10, Iteration 150, loss = 0.0391\n",
            "\n",
            "Epoch: 10, Iteration 200, loss = 0.0364\n",
            "\n",
            "RMSE: 0.22277724338932847 Pearson 0.9714642669224183\n",
            "\n",
            "Epoch: 11, Iteration 0, loss = 0.0485\n",
            "\n",
            "Epoch: 11, Iteration 50, loss = 0.0172\n",
            "\n",
            "Epoch: 11, Iteration 100, loss = 0.0513\n",
            "\n",
            "Epoch: 11, Iteration 150, loss = 0.0211\n",
            "\n",
            "Epoch: 11, Iteration 200, loss = 0.0300\n",
            "\n",
            "RMSE: 0.19745012124249256 Pearson 0.9774889739043854\n",
            "\n",
            "Epoch: 12, Iteration 0, loss = 0.0398\n",
            "\n",
            "Epoch: 12, Iteration 50, loss = 0.0134\n",
            "\n",
            "Epoch: 12, Iteration 100, loss = 0.0389\n",
            "\n",
            "Epoch: 12, Iteration 150, loss = 0.0157\n",
            "\n",
            "Epoch: 12, Iteration 200, loss = 0.0267\n",
            "\n",
            "RMSE: 0.17746615036332405 Pearson 0.9818156568625545\n",
            "\n",
            "Epoch: 13, Iteration 0, loss = 0.0323\n",
            "\n",
            "Epoch: 13, Iteration 50, loss = 0.0115\n",
            "\n",
            "Epoch: 13, Iteration 100, loss = 0.0293\n",
            "\n",
            "Epoch: 13, Iteration 150, loss = 0.0133\n",
            "\n",
            "Epoch: 13, Iteration 200, loss = 0.0230\n",
            "\n",
            "RMSE: 0.16035370000518565 Pearson 0.985166471151723\n",
            "\n",
            "Epoch: 14, Iteration 0, loss = 0.0259\n",
            "\n",
            "Epoch: 14, Iteration 50, loss = 0.0099\n",
            "\n",
            "Epoch: 14, Iteration 100, loss = 0.0210\n",
            "\n",
            "Epoch: 14, Iteration 150, loss = 0.0118\n",
            "\n",
            "Epoch: 14, Iteration 200, loss = 0.0193\n",
            "\n",
            "RMSE: 0.1455242385304988 Pearson 0.9877864879584185\n",
            "\n",
            "Epoch: 15, Iteration 0, loss = 0.0207\n",
            "\n",
            "Epoch: 15, Iteration 50, loss = 0.0085\n",
            "\n",
            "Epoch: 15, Iteration 100, loss = 0.0162\n",
            "\n",
            "Epoch: 15, Iteration 150, loss = 0.0106\n",
            "\n",
            "Epoch: 15, Iteration 200, loss = 0.0158\n",
            "\n",
            "RMSE: 0.1321494417519597 Pearson 0.9899422934627365\n",
            "\n",
            "Epoch: 16, Iteration 0, loss = 0.0163\n",
            "\n",
            "Epoch: 16, Iteration 50, loss = 0.0073\n",
            "\n",
            "Epoch: 16, Iteration 100, loss = 0.0129\n",
            "\n",
            "Epoch: 16, Iteration 150, loss = 0.0096\n",
            "\n",
            "Epoch: 16, Iteration 200, loss = 0.0127\n",
            "\n",
            "RMSE: 0.11980811044130801 Pearson 0.9917434665845796\n",
            "\n",
            "Epoch: 17, Iteration 0, loss = 0.0127\n",
            "\n",
            "Epoch: 17, Iteration 50, loss = 0.0061\n",
            "\n",
            "Epoch: 17, Iteration 100, loss = 0.0102\n",
            "\n",
            "Epoch: 17, Iteration 150, loss = 0.0087\n",
            "\n",
            "Epoch: 17, Iteration 200, loss = 0.0099\n",
            "\n",
            "RMSE: 0.10834253217223612 Pearson 0.9932625440597556\n",
            "\n",
            "Epoch: 18, Iteration 0, loss = 0.0098\n",
            "\n",
            "Epoch: 18, Iteration 50, loss = 0.0050\n",
            "\n",
            "Epoch: 18, Iteration 100, loss = 0.0072\n",
            "\n",
            "Epoch: 18, Iteration 150, loss = 0.0078\n",
            "\n",
            "Epoch: 18, Iteration 200, loss = 0.0076\n",
            "\n",
            "RMSE: 0.09765226561721835 Pearson 0.9945480054194378\n",
            "\n",
            "Epoch: 19, Iteration 0, loss = 0.0074\n",
            "\n",
            "Epoch: 19, Iteration 50, loss = 0.0041\n",
            "\n",
            "Epoch: 19, Iteration 100, loss = 0.0046\n",
            "\n",
            "Epoch: 19, Iteration 150, loss = 0.0070\n",
            "\n",
            "Epoch: 19, Iteration 200, loss = 0.0057\n",
            "\n",
            "RMSE: 0.08777083470087314 Pearson 0.995619280124368\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtCNGiJphD6i",
        "colab_type": "text"
      },
      "source": [
        "### Validation Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x7jW2dQgWYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8c97aa78-32cb-49b8-bfdf-da7ffc54002f"
      },
      "source": [
        "def check_performance(loader, model):\n",
        "    \"\"\"\n",
        "    Method to check the performance on validation set.\n",
        "\n",
        "    Args: \n",
        "        loader: the dataloder which contains the train data \n",
        "        model: the model to be trained.\n",
        "    \n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "\n",
        "            x = x.to(device=device, dtype=dtype)\n",
        "            y = y.to(device=device, dtype=float)\n",
        "\n",
        "            score = model(x)\n",
        "\n",
        "            predictions.append(score.cpu().detach().numpy())\n",
        "    \n",
        "    predictions = np.asarray(predictions).ravel()\n",
        "\n",
        "    pearson = pearsonr(score_validation, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,score_validation)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\n",
        "check_performance(loader_val, baseline_model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.9572063781921932 Pearson 0.2430570430092266\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}