{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN+Word_embed_GloVe_Word2Vec+pre_process.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "B9y5BMrRIVZC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWVNrSepufI",
        "colab_type": "text"
      },
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m-5YiHHftBX",
        "colab_type": "text"
      },
      "source": [
        "## Dowdload and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNdY9X-fSlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A0Z6AWU37Gg",
        "colab_type": "text"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w8HNiip39OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path):\n",
        "    \"\"\"\n",
        "    Read data from the data path.\n",
        "\n",
        "    Args: \n",
        "        path: the path of the dataset, normally in local folder.\n",
        "    \n",
        "    Returns:\n",
        "        Loaded raw dataset. \n",
        "    \"\"\"\n",
        "    with open(path) as dataset:\n",
        "        raw_data = dataset.readlines()\n",
        "\n",
        "    return raw_data\n",
        "\n",
        "\n",
        "# Define the path of the train dataset\n",
        "english_train_path = \"train.enzh.src\"\n",
        "chinese_train_path = \"train.enzh.mt\"   \n",
        "scores_train_path = \"train.enzh.scores\"\n",
        "# Define the path of the validatin dataset\n",
        "english_validation_path = \"dev.enzh.src\"\n",
        "chinese_validation_path = \"dev.enzh.mt\"   \n",
        "scores_validation_path = \"dev.enzh.scores\"\n",
        "# Define the path of the test dataset\n",
        "english_test_path = \"test.enzh.src\"\n",
        "chinese_test_path = \"test.enzh.mt\"\n",
        "\n",
        "\n",
        "# Read train, validation, test data\n",
        "raw_english_train = read_data(english_train_path)\n",
        "raw_chinese_train = read_data(chinese_train_path)\n",
        "raw_english_validation = read_data(english_validation_path)\n",
        "raw_chinese_validation = read_data(chinese_validation_path)\n",
        "raw_english_test = read_data(english_test_path)\n",
        "raw_chinese_test = read_data(chinese_test_path)\n",
        "\n",
        "# read scores for train and validation dataset \n",
        "score_train = read_data(scores_train_path)\n",
        "score_validation = read_data(scores_validation_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn4WhYx88zdb",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIRQMw-u83xn",
        "colab_type": "text"
      },
      "source": [
        "### English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhrn7nlJ84wU",
        "colab_type": "text"
      },
      "source": [
        "Download and Import:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzE-g3am861e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "fc626e70-82f7-42ac-9ea1-18af50ccad10"
      },
      "source": [
        "import spacy\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# RUN ONCE\n",
        "\n",
        "# Downloading spacy models for english\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300\n",
        "\n",
        "# downloading stopwords from the nltk package\n",
        "download('stopwords') # stopwords dictionary"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 799kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=f882d519ae8fd9f841b53ffb3ff7cdd511c0cc9ed9709033b243aff4cf261397\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-io9i_mdy/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJwKGVLZ8-2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer model\n",
        "nlp_en = spacy.load('en300')\n",
        "\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_en(raw_corpus):\n",
        "    \"\"\"\n",
        "    Method to preprocesss raw English corpus, including lower casing, stop word \n",
        "    removal, etc.\n",
        "\n",
        "    Args: \n",
        "        raw_corpus: the raw dataset needs to be processed.\n",
        "    \n",
        "    Returns:\n",
        "        The processed corpus.\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sentence in raw_corpus:\n",
        "        text = sentence.lower()\n",
        "        doc = [token.lemma_ for token in  nlp_en.tokenizer(text)]\n",
        "        doc = [word for word in doc if word not in stop_words_en]\n",
        "        doc = [word for word in doc if word.isalpha()] # restricts string to alphabetic characters only\n",
        "        preprocessed_corpus.append(\" \".join(doc))\n",
        "    return preprocessed_corpus\n",
        "\n",
        "\n",
        "# Preprocess the train, validation, test dataset.\n",
        "preprocessed_english_train = preprocess_en(raw_english_train)\n",
        "preprocessed_english_validation = preprocess_en(raw_english_validation)\n",
        "preprocessed_english_test = preprocess_en(raw_english_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoBS-P4I9Bsw",
        "colab_type": "text"
      },
      "source": [
        "### Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP97zDef9Cr1",
        "colab_type": "text"
      },
      "source": [
        "Download and Import:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vODbR_SA9G0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "dd3352a8-e7df-46cd-f2e8-dff4f28306f3"
      },
      "source": [
        "# Download the package used to process Chinese\n",
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "import jieba"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 13:22:05--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "chinese_stop_words.     [   <=>              ] 419.57K   471KB/s    in 0.9s    \n",
            "\n",
            "2020-02-28 13:22:07 (471 KB/s) - ‘chinese_stop_words.txt’ saved [429642]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2XrlpRb9Ir-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "f71a6049-1196-40c5-a79f-ab8ff12e6675"
      },
      "source": [
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "\n",
        "def processing_zh(raw_corpus):\n",
        "    \"\"\"\n",
        "    Method to preprocesss Chinese corpus, tokenization, stop word removal, etc.\n",
        "\n",
        "    Args: \n",
        "        raw_corpus: the raw dataset needs to be processed.\n",
        "    \n",
        "    Returns:\n",
        "        The processed corpus.\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sentence in raw_corpus:\n",
        "        # seg_list = jieba.lcut(sentence,cut_all=True) # full mode\n",
        "        seg_list = jieba.lcut(sentence) # precise mode\n",
        "        doc = [word for word in seg_list if word not in stop_words]\n",
        "        docs = [e for e in doc if e.isalnum()]\n",
        "        preprocessed_corpus.append(\" \".join(docs))\n",
        "    return preprocessed_corpus\n",
        "\n",
        "# Preprocess the train, validation, test dataset.\n",
        "preprocessed_chinese_train = processing_zh(raw_chinese_train)\n",
        "preprocessed_chinese_validation = processing_zh(raw_chinese_validation)\n",
        "preprocessed_chinese_test = processing_zh(raw_chinese_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.912 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjWBbJErJ9ku",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding - Word2Vec and GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9GB8pkxMWY9",
        "colab_type": "text"
      },
      "source": [
        "English:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLRl-K9qMbNi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4cab5e26-7f59-4f2d-a32e-b6851496d64d"
      },
      "source": [
        "import torchtext\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "def get_word_vector(word):\n",
        "    \"\"\"\n",
        "    Method to get word vector from glove model.\n",
        "\n",
        "    Args: \n",
        "        word: the original word.\n",
        "    \n",
        "    Returns:\n",
        "        The word vector.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        vec = glove.vectors[glove.stoi[word]]\n",
        "        if vec is not None:\n",
        "            return vec\n",
        "    except KeyError:\n",
        "      # print(f\"Word {word} does not exist\")\n",
        "        return torch.zeros((100))\n",
        "\n",
        "def get_sentence_vector(line):\n",
        "    \"\"\"\n",
        "    Method to get sentence vector for each line.\n",
        "\n",
        "    Args: \n",
        "        line: the original line.\n",
        "    \n",
        "    Returns:\n",
        "        The sentence vector.\n",
        "    \"\"\"\n",
        "\n",
        "    vectors = []\n",
        "    for w in line:\n",
        "        emb = get_word_vector(w)\n",
        "        # do not add if the word is out of vocabulary\n",
        "        if emb is not None:\n",
        "            vectors.append(emb)\n",
        "    \n",
        "    return np.stack(vectors)\n",
        "\n",
        "\n",
        "def get_embeddings_en(corpus):\n",
        "    \"\"\"\n",
        "    Method to get English embedding for the corpus.\n",
        "\n",
        "    Args: \n",
        "        corpus: the original line.\n",
        "    \n",
        "    Returns:\n",
        "        The sentence vector.\n",
        "    \"\"\"\n",
        "    sentences_vectors =[]\n",
        "    for l in corpus:\n",
        "        try:\n",
        "            vec = get_sentence_vector(l)\n",
        "            sentences_vectors.append(vec)\n",
        "        except:\n",
        "            sentences_vectors.append(0)\n",
        "\n",
        "    return sentences_vectors"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:31, 2.20MB/s]                           \n",
            "100%|█████████▉| 398893/400000 [00:22<00:00, 18721.21it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQD_sEWCMUqw",
        "colab_type": "text"
      },
      "source": [
        "Chinese:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVPDDKVLXcRE",
        "colab_type": "code",
        "outputId": "b0499562-0f92-416c-fc20-12021cc35aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "!wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "!unzip zh.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 13:29:54--  http://vectors.nlpl.eu/repository/20/35.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1458485917 (1.4G) [application/zip]\n",
            "Saving to: ‘zh.zip’\n",
            "\n",
            "zh.zip                5%[>                   ]  73.57M  9.36MB/s    eta 4m 6s  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 398893/400000 [00:40<00:00, 18721.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "zh.zip              100%[===================>]   1.36G  9.40MB/s    in 2m 34s  \n",
            "\n",
            "2020-02-28 13:32:30 (9.02 MB/s) - ‘zh.zip’ saved [1458485917/1458485917]\n",
            "\n",
            "Archive:  zh.zip\n",
            "  inflating: LIST                    \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0xXj6xlKIwe",
        "colab_type": "code",
        "outputId": "45567dff-2936-4094-e3be-1d94d829d3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import gensim \n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True) \n",
        "\n",
        "def get_sentence_vector_zh(line):\n",
        "    \"\"\"\n",
        "    Method to get sentence vector for each line.\n",
        "\n",
        "    Args: \n",
        "        line: the original line.\n",
        "    \n",
        "    Returns:\n",
        "        The sentence vector.\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    for w in line:\n",
        "        try:\n",
        "            emb = wv_from_bin[w]\n",
        "            vectors.append(emb)\n",
        "        except:\n",
        "            emb = np.zeros(100)\n",
        "            vectors.append(emb)\n",
        "    if vectors:\n",
        "        return np.stack(vectors)\n",
        "    else:\n",
        "        return np.zeros((100,))\n",
        "\n",
        "\n",
        "def get_embeddings_zh(corpus):\n",
        "    \"\"\"\n",
        "    Method to get English embedding for the corpus.\n",
        "\n",
        "    Args: \n",
        "        corpus: the original line.\n",
        "    \n",
        "    Returns:\n",
        "        The sentence vector.\n",
        "    \"\"\"\n",
        "    sentences_vectors =[]\n",
        "    for l in corpus:\n",
        "        vec = get_sentence_vector_zh(l)\n",
        "        if vec is not None:\n",
        "            sentences_vectors.append(vec)\n",
        "        else:\n",
        "            print(l)\n",
        "    return sentences_vectors"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujF33cElOekr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data into word embedding.\n",
        "word_embeddings_train_en = get_embeddings_en(preprocessed_english_train) \n",
        "word_embeddings_train_zh = get_embeddings_zh(preprocessed_chinese_train) \n",
        "\n",
        "word_embeddings_val_en = get_embeddings_en(preprocessed_english_validation)\n",
        "word_embeddings_val_zh = get_embeddings_zh(preprocessed_chinese_validation)\n",
        "\n",
        "score_train = np.asarray(score_train).astype(float)\n",
        "score_validation = np.asarray(score_validation).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLSCdfUK2FSJ",
        "colab_type": "text"
      },
      "source": [
        "## Model: Recurrent Neural Network with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VttQirg4O5k4",
        "colab_type": "text"
      },
      "source": [
        "LSTM takes in word embeddings(Word2Vec and GloVe) to capture the order within a sentnce sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au2zIsPH2czA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feed data into data loader\n",
        "lstm_train = []\n",
        "lstm_val = []\n",
        "for i in range(len(word_embeddings_train_en)):\n",
        "    lstm_train.append([word_embeddings_train_en[i], word_embeddings_train_zh[i], score_train[i]])\n",
        "\n",
        "for i in range(len(word_embeddings_val_en)):\n",
        "    lstm_val.append([word_embeddings_val_en[i], word_embeddings_val_zh[i], score_validation[i]])\n",
        "\n",
        "batch_size = 32\n",
        "loader_train = torch.utils.data.DataLoader(lstm_train, batch_size=batch_size)\n",
        "loader_val = torch.utils.data.DataLoader(lstm_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbUe94WQYa6N",
        "colab_type": "code",
        "outputId": "0ca4d0fd-48bc-4928-ae0f-01d7643ae697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # we will be using float throughout this tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constant to control how frequently we print train loss\n",
        "print_every = 50\n",
        "\n",
        "print('using device:', device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ80tfvOF1DR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Neural Network with LSTM.\n",
        "    2 LSTM Layers \n",
        "    1 fully connected neural network\n",
        "    1 output layer\n",
        "\n",
        "    Attributes:\n",
        "        lstm_en: hidden lstm layer for English\n",
        "        lstm_cn: hidden lstm layer for Chinese\n",
        "        fc1: fully connected layer for regression\n",
        "        out: the output of the model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.lstm_en = nn.LSTM(100, 32, batch_first=True)\n",
        "        self.lstm_cn = nn.LSTM(100, 32, batch_first=True)\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.out = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, cn, en):\n",
        "        lstm_out_en, _ = self.lstm_en(en.view(1,-1,100))\n",
        "        lstm_out_cn, _ = self.lstm_cn(cn.view(1,-1,100))\n",
        "\n",
        "        fc_en = lstm_out_en[:,-1,:]\n",
        "        fc_cn = lstm_out_cn[:,-1,:]\n",
        "        fc_input = torch.cat((fc_en,fc_cn),1)\n",
        "\n",
        "        f1 = self.fc1(fc_input)\n",
        "        out = self.out(f1)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSDr46ktISul",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmApNsVxF3Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "def check_train_performance(model):\n",
        "    \"\"\"\n",
        "    Method to check the performance of the train process.\n",
        "\n",
        "    Args: \n",
        "        loader: the dataloder which contains the train data \n",
        "        model: the model to be trained.\n",
        "    \n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(word_embeddings_train_en)):\n",
        "            x = torch.tensor(word_embeddings_train_en[i])\n",
        "            y = torch.tensor(word_embeddings_train_zh[i])\n",
        "            z = torch.tensor(score_train[i])\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=dtype)\n",
        "            z = z.to(device=device, dtype=torch.float)\n",
        "\n",
        "            score = model(x, y)\n",
        "            \n",
        "\n",
        "            predictions.append(score.cpu().detach().numpy())\n",
        "\n",
        "    predictions = np.asarray([i for item in predictions for i in item]).squeeze(1)\n",
        "    \n",
        "    pearson = pearsonr(score_train, predictions)\n",
        "    print(f'RMSE: {rmse(predictions, score_train)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "def train_part(model, optimizer, scheduler=None, epochs=1):\n",
        "    \"\"\"\n",
        "    Method to train the model.\n",
        "\n",
        "    Args: \n",
        "        model: the model to be trained.\n",
        "        optimizer: the optimizer used for optimisation.\n",
        "        scheduler: the scheduler used in this training process\n",
        "        epochs: the number of epochs , default is 1\n",
        "    \n",
        "    \"\"\"\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        #for t, (x, y) in enumerate(loader_train):\n",
        "        for i in range(len(word_embeddings_train_en)):\n",
        "            x = torch.tensor(word_embeddings_train_en[i])\n",
        "            y = torch.tensor(word_embeddings_train_zh[i])\n",
        "            z = torch.tensor(score_train[i])\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=dtype)\n",
        "            z = z.to(device=device, dtype=torch.float)\n",
        "\n",
        "\n",
        "            scores = model(x, y)\n",
        "\n",
        "            scores = scores.squeeze(1)\n",
        "\n",
        "            loss = F.mse_loss(scores, z)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            #if i % print_every == 0:\n",
        "               # print('Epoch: %d, Iteration %d, loss = %.4f' % (e, i, loss.item()))\n",
        "                #check_accuracy(loader_val, model)\n",
        "               # print()\n",
        "        check_train_performance(model)\n",
        "        # Adjust the learning rate\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uCNMWsTF46S",
        "colab_type": "code",
        "outputId": "20bda5c8-5351-4901-a197-7830a110068e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        }
      },
      "source": [
        "lstm_model = LSTM()\n",
        "print(lstm_model)\n",
        "optimizer = optim.Adam(lstm_model.parameters())\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 10)\n",
        "train_part(lstm_model, optimizer, scheduler, epochs=20)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM(\n",
            "  (lstm_en): LSTM(100, 32, batch_first=True)\n",
            "  (lstm_cn): LSTM(100, 32, batch_first=True)\n",
            "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.9057440071183247 Pearson 0.2521790764573601\n",
            "\n",
            "RMSE: 0.9000784628598647 Pearson 0.2847856949002749\n",
            "\n",
            "RMSE: 0.897998146600625 Pearson 0.30976558223653033\n",
            "\n",
            "RMSE: 0.8863858058103699 Pearson 0.3505853426084062\n",
            "\n",
            "RMSE: 0.8713254110486406 Pearson 0.39863721281214903\n",
            "\n",
            "RMSE: 0.8530323569733004 Pearson 0.4333885149137533\n",
            "\n",
            "RMSE: 0.8404054320514607 Pearson 0.468044271124997\n",
            "\n",
            "RMSE: 0.8336450881728089 Pearson 0.48850841347079493\n",
            "\n",
            "RMSE: 0.8276255400554453 Pearson 0.5101086370945245\n",
            "\n",
            "RMSE: 0.8176153942151702 Pearson 0.5306453914350168\n",
            "\n",
            "RMSE: 0.724504688302701 Pearson 0.6268259808005847\n",
            "\n",
            "RMSE: 0.710854920789603 Pearson 0.644266663610362\n",
            "\n",
            "RMSE: 0.7001807537072474 Pearson 0.6573975606943294\n",
            "\n",
            "RMSE: 0.6906660895339831 Pearson 0.6688136545528136\n",
            "\n",
            "RMSE: 0.6817020411934557 Pearson 0.6793101024854478\n",
            "\n",
            "RMSE: 0.6730130078652536 Pearson 0.6892299347939951\n",
            "\n",
            "RMSE: 0.6644910433519302 Pearson 0.6987170443552866\n",
            "\n",
            "RMSE: 0.6560631482526605 Pearson 0.7078689058337039\n",
            "\n",
            "RMSE: 0.6476640637026668 Pearson 0.7167651787086037\n",
            "\n",
            "RMSE: 0.6392552168882538 Pearson 0.7254526556115055\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9y5BMrRIVZC",
        "colab_type": "text"
      },
      "source": [
        "### Validation Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE37zmMaF6nz",
        "colab_type": "code",
        "outputId": "0c3914d2-6a89-4b67-e15b-cfa6d34b2fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def val_performance_check(model):\n",
        "    \"\"\"\n",
        "    Method to check the performance of the validation set.\n",
        "\n",
        "    Args: \n",
        "        model: the model after training.\n",
        "    \n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(word_embeddings_val_en)):\n",
        "            x = torch.tensor(word_embeddings_val_en[i])\n",
        "            y = torch.tensor(word_embeddings_val_zh[i])\n",
        "            z = torch.tensor(score_validation[i])\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=dtype)\n",
        "            z = z.to(device=device, dtype=torch.float)\n",
        "\n",
        "            score = model(x, y)\n",
        "            \n",
        "\n",
        "            predictions.append(score.cpu().detach().numpy())\n",
        "\n",
        "    predictions = np.asarray([i for item in predictions for i in item]).squeeze(1)\n",
        "    \n",
        "    pearson = pearsonr(score_validation, predictions)\n",
        "    print(f'RMSE: {rmse(predictions, score_validation)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "val_performance_check(lstm_model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 1.037653168591964 Pearson 0.15092400248407953\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}