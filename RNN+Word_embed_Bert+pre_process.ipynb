{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN+Word_embed_Bert+pre_process.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWVNrSepufI",
        "colab_type": "text"
      },
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m-5YiHHftBX",
        "colab_type": "text"
      },
      "source": [
        "## Dowdload and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNdY9X-fSlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A0Z6AWU37Gg",
        "colab_type": "text"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w8HNiip39OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path):\n",
        "    \"\"\"\n",
        "    Read data from the data path.\n",
        "\n",
        "    Args: \n",
        "        path: the path of the dataset, normally in local folder.\n",
        "    \n",
        "    Returns:\n",
        "        Loaded raw dataset. \n",
        "    \"\"\"\n",
        "    with open(path) as dataset:\n",
        "        raw_data = dataset.readlines()\n",
        "\n",
        "    return raw_data\n",
        "\n",
        "\n",
        "# Define the path of the train dataset\n",
        "english_train_path = \"train.enzh.src\"\n",
        "chinese_train_path = \"train.enzh.mt\"   \n",
        "scores_train_path = \"train.enzh.scores\"\n",
        "# Define the path of the validatin dataset\n",
        "english_validation_path = \"dev.enzh.src\"\n",
        "chinese_validation_path = \"dev.enzh.mt\"   \n",
        "scores_validation_path = \"dev.enzh.scores\"\n",
        "# Define the path of the test dataset\n",
        "english_test_path = \"test.enzh.src\"\n",
        "chinese_test_path = \"test.enzh.mt\"\n",
        "\n",
        "\n",
        "# Read train, validation, test data\n",
        "raw_english_train = read_data(english_train_path)\n",
        "raw_chinese_train = read_data(chinese_train_path)\n",
        "raw_english_validation = read_data(english_validation_path)\n",
        "raw_chinese_validation = read_data(chinese_validation_path)\n",
        "raw_english_test = read_data(english_test_path)\n",
        "raw_chinese_test = read_data(chinese_test_path)\n",
        "\n",
        "# read scores for train and validation dataset \n",
        "score_train = read_data(scores_train_path)\n",
        "score_validation = read_data(scores_validation_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYWFTxU93iOw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Pre-processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYHSKMdWbFRD",
        "colab_type": "text"
      },
      "source": [
        "### English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UhtvsNzWjNl",
        "colab_type": "text"
      },
      "source": [
        "Download and Import:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaLVCGyeWvFm",
        "colab_type": "code",
        "outputId": "4423f122-7e71-4464-c58e-e39b165e694d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "import spacy\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# RUN ONCE\n",
        "\n",
        "# Downloading spacy models for english\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300\n",
        "\n",
        "# downloading stopwords from the nltk package\n",
        "download('stopwords') # stopwords dictionary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 1.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=148c3e7bf235453bc92d415fdd04af8445434683009132b3f04b49cca3b9a17e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ccx2r4ja/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDd8yUP23cyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer model\n",
        "nlp_en = spacy.load('en300')\n",
        "\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_en(raw_corpus):\n",
        "    \"\"\"\n",
        "    Method to preprocesss raw English corpus, including lower casing, stop word \n",
        "    removal, etc.\n",
        "\n",
        "    Args: \n",
        "        raw_corpus: the raw dataset needs to be processed.\n",
        "    \n",
        "    Returns:\n",
        "        The processed corpus.\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sentence in raw_corpus:\n",
        "        text = sentence.lower()\n",
        "        doc = [token.lemma_ for token in  nlp_en.tokenizer(text)]\n",
        "        doc = [word for word in doc if word not in stop_words_en]\n",
        "        doc = [word for word in doc if word.isalpha()] # restricts string to alphabetic characters only\n",
        "        preprocessed_corpus.append(\" \".join(doc))\n",
        "    return preprocessed_corpus\n",
        "\n",
        "\n",
        "# Preprocess the train, validation, test dataset.\n",
        "preprocessed_english_train = preprocess_en(raw_english_train)\n",
        "preprocessed_english_validation = preprocess_en(raw_english_validation)\n",
        "preprocessed_english_test = preprocess_en(raw_english_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQAAEU18CUWT",
        "colab_type": "text"
      },
      "source": [
        "### Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0tJbVXQWVTV",
        "colab_type": "text"
      },
      "source": [
        "Download and Import:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfzmTrENWWfS",
        "colab_type": "code",
        "outputId": "31cac817-891a-4fce-806b-888dd70ecb0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# Download the package used to process Chinese\n",
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
        "\n",
        "import jieba"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 21:49:04--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 13.250.177.223\n",
            "Connecting to github.com (github.com)|13.250.177.223|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "\rchinese_stop_words.     [<=>                 ]       0  --.-KB/s               \rchinese_stop_words.     [ <=>                ] 417.17K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-27 21:49:05 (15.3 MB/s) - ‘chinese_stop_words.txt’ saved [427178]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9r-p5eKCfub",
        "colab_type": "code",
        "outputId": "11001bd8-330e-4309-8120-915b97c50b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "\n",
        "def processing_zh(raw_corpus):\n",
        "    \"\"\"\n",
        "    Method to preprocesss Chinese corpus, tokenization, stop word removal, etc.\n",
        "\n",
        "    Args: \n",
        "        raw_corpus: the raw dataset needs to be processed.\n",
        "    \n",
        "    Returns:\n",
        "        The processed corpus.\n",
        "    \"\"\"\n",
        "    preprocessed_corpus = []\n",
        "    for sentence in raw_corpus:\n",
        "        # seg_list = jieba.lcut(sentence,cut_all=True) # full mode\n",
        "        seg_list = jieba.lcut(sentence) # precise mode\n",
        "        doc = [word for word in seg_list if word not in stop_words]\n",
        "        docs = [e for e in doc if e.isalnum()]\n",
        "        preprocessed_corpus.append(\" \".join(docs))\n",
        "    return preprocessed_corpus\n",
        "\n",
        "# Preprocess the train, validation, test dataset.\n",
        "preprocessed_chinese_train = processing_zh(raw_chinese_train)\n",
        "preprocessed_chinese_validation = processing_zh(raw_chinese_validation)\n",
        "preprocessed_chinese_test = processing_zh(raw_chinese_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.798 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej1vYxsTFdH3",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding - Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Eo05N-2Jbz",
        "colab_type": "text"
      },
      "source": [
        "English (preprocessed corpus):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gfpuJL12Ha8",
        "colab_type": "code",
        "outputId": "282c1d57-0f8a-4175-f8ad-57c13baad79c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "source": [
        "# download pretrained BERT model cased_L-12_H-768_A-12\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "!unzip cased_L-12_H-768_A-12.zip\n",
        "\n",
        "!pip install bert-serving-server\n",
        "!pip install bert-serving-client\n",
        "!nohup bert-serving-start -model_dir=./cased_L-12_H-768_A-12 > out.file 2>&1 &"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 21:49:32--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.24.128, 2404:6800:4003:c00::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.24.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M  41.6MB/s    in 9.9s    \n",
            "\n",
            "2020-02-27 21:49:42 (38.9 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n",
            "Collecting bert-serving-server\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \n",
            "\u001b[?25hCollecting pyzmq>=17.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/fa/e6e10410f01b03d10ab0705717d1246f63cdbbc0676140c78f0f757db332/pyzmq-19.0.0-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 23.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.12.0)\n",
            "Collecting GPUtil>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.17.5)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.1.0)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=59706a8daf7332ce6baf8cc0fd68aa8376b353ff9acbd393cc1bebb986473d70\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: pyzmq, GPUtil, bert-serving-server\n",
            "  Found existing installation: pyzmq 17.0.0\n",
            "    Uninstalling pyzmq-17.0.0:\n",
            "      Successfully uninstalled pyzmq-17.0.0\n",
            "Successfully installed GPUtil-1.4.0 bert-serving-server-1.10.0 pyzmq-19.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "zmq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-serving-client\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/09/aae1405378a848b2e87769ad89a43d6d71978c4e15534ca48e82e723a72f/bert_serving_client-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (19.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (1.17.5)\n",
            "Installing collected packages: bert-serving-client\n",
            "Successfully installed bert-serving-client-1.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo3Gc-h52zWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert_serving.client import BertClient\n",
        "bc_en = BertClient(show_server_config=False)\n",
        "# encode English preprocessed corpus by word\n",
        "english_validation_wbert = []\n",
        "for i in range(len(preprocessed_english_validation)):\n",
        "  sentence = preprocessed_english_validation[i].split()\n",
        "  word_embedding = bc_en.encode(sentence)\n",
        "  english_validation_wbert.append(word_embedding)\n",
        "english_validation_wbert_np = np.array(english_validation_wbert)\n",
        "np.save('english_validation_wbert.npy', english_validation_wbert_np)\n",
        "\n",
        "english_train_wbert = []\n",
        "for i in range(len(preprocessed_english_train)):\n",
        "  sentence = preprocessed_english_train[i].split()\n",
        "  word_embedding = bc_en.encode(sentence)\n",
        "  english_train_wbert.append(word_embedding)\n",
        "english_train_wbert_np = np.array(english_train_wbert)\n",
        "np.save('english_train_wbert.npy', english_train_wbert_np)\n",
        "\n",
        "english_test_wbert = []\n",
        "for i in range(len(preprocessed_english_test)):\n",
        "  sentence = preprocessed_english_test[i].split()\n",
        "  word_embedding = bc_en.encode(sentence)\n",
        "  english_test_wbert.append(word_embedding)\n",
        "english_test_wbert_np = np.array(english_test_wbert)\n",
        "np.save('english_test_wbert.npy', english_test_wbert_np)\n",
        "\n",
        "bc_en.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZQ6ZBqF3CW5",
        "colab_type": "text"
      },
      "source": [
        "Chinese (preprocessed corpus):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hww5oDet3IC4",
        "colab_type": "code",
        "outputId": "d4b736c3-e2b1-4c81-8ff4-9217ec3f874f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "# download pretrained BERT model chinese_L-12_H-768_A-12\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
        "!unzip chinese_L-12_H-768_A-12.zip\n",
        "\n",
        "!pip install bert-serving-server\n",
        "!pip install bert-serving-client\n",
        "!nohup bert-serving-start -model_dir=./chinese_L-12_H-768_A-12 > out.file 2>&1 &"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 21:53:39--  https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.128, 2404:6800:4003:c03::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381892918 (364M) [application/zip]\n",
            "Saving to: ‘chinese_L-12_H-768_A-12.zip’\n",
            "\n",
            "chinese_L-12_H-768_ 100%[===================>] 364.20M  28.6MB/s    in 14s     \n",
            "\n",
            "2020-02-27 21:53:55 (26.0 MB/s) - ‘chinese_L-12_H-768_A-12.zip’ saved [381892918/381892918]\n",
            "\n",
            "Archive:  chinese_L-12_H-768_A-12.zip\n",
            "   creating: chinese_L-12_H-768_A-12/\n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: chinese_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_config.json  \n",
            "Requirement already satisfied: bert-serving-server in /usr/local/lib/python3.6/dist-packages (1.10.0)\n",
            "Requirement already satisfied: GPUtil>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.4.0)\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (19.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.17.5)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.1.0)\n",
            "Requirement already satisfied: bert-serving-client in /usr/local/lib/python3.6/dist-packages (1.10.0)\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (19.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Rkkiz-b3SLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert_serving.client import BertClient\n",
        "bc_ch = BertClient(show_server_config=False)\n",
        "# encode Chinese preprocessed corpus by word\n",
        "chinese_validation_wbert = []\n",
        "for i in range(len(preprocessed_chinese_validation)):\n",
        "  sentence = preprocessed_chinese_validation[i].split()\n",
        "  word_embedding = bc_ch.encode(sentence)\n",
        "  chinese_validation_wbert.append(word_embedding)\n",
        "chinese_validation_wbert_np = np.array(chinese_validation_wbert)\n",
        "np.save('chinese_validation_wbert.npy', chinese_validation_wbert_np)\n",
        "\n",
        "chinese_train_wbert = []\n",
        "for i in range(len(preprocessed_chinese_train)):\n",
        "  sentence = preprocessed_chinese_train[i].split()\n",
        "  word_embedding = bc_ch.encode(sentence)\n",
        "  chinese_train_wbert.append(word_embedding)\n",
        "chinese_train_wbert_np = np.array(chinese_train_wbert)\n",
        "np.save('chinese_train_wbert.npy', chinese_train_wbert_np)\n",
        "\n",
        "chinese_test_wbert = []\n",
        "for i in range(len(preprocessed_chinese_test)):\n",
        "  sentence = preprocessed_chinese_test[i].split()\n",
        "  word_embedding = bc_ch.encode(sentence)\n",
        "  chinese_test_wbert.append(word_embedding)\n",
        "chinese_test_wbert_np = np.array(chinese_test_wbert)\n",
        "np.save('chinese_test_wbert.npy', chinese_test_wbert_np)\n",
        "\n",
        "bc_ch.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvXG-pnJuaj6",
        "colab_type": "text"
      },
      "source": [
        "## Model: LSTM (BERT word embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSYix6sGu23I",
        "colab_type": "code",
        "outputId": "6d53489f-884f-4e7a-a6a1-2c7d3a45e72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yofay5GvtKv",
        "colab_type": "code",
        "outputId": "e77935ba-b485-456b-aaf1-846d3aa5a3db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/BERT word embedding\"\n",
        "english_train_bert = np.load('/content/drive/My Drive/BERT word embedding/english_train_wbert.npy', allow_pickle=True)\n",
        "chinese_train_bert = np.load('/content/drive/My Drive/BERT word embedding/chinese_train_wbert.npy', allow_pickle=True)\n",
        "english_val_bert = np.load('/content/drive/My Drive/BERT word embedding/english_validation_wbert.npy', allow_pickle=True)\n",
        "chinese_val_bert = np.load('/content/drive/My Drive/BERT word embedding/chinese_validation_wbert.npy', allow_pickle=True)\n",
        "chinese_test_bert = np.load('/content/drive/My Drive/BERT word embedding/chinese_test_wbert.npy', allow_pickle=True)\n",
        "english_test_bert = np.load('/content/drive/My Drive/BERT word embedding/english_test_wbert.npy', allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/My Drive/BERT word embedding': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9ac6077ffc16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls \"/content/drive/My Drive/BERT word embedding\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menglish_train_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/BERT word embedding/english_train_wbert.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mchinese_train_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/BERT word embedding/chinese_train_wbert.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menglish_val_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/BERT word embedding/english_validation_wbert.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchinese_val_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/BERT word embedding/chinese_validation_wbert.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/BERT word embedding/english_train_wbert.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgAUiCHRzN0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_with_BERT(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Neural Network with LSTM.\n",
        "    2 LSTM Layers \n",
        "    4 fully connected neural network\n",
        "    1 output layer\n",
        "\n",
        "    Attributes:\n",
        "        lstm_en: hidden lstm layer for English\n",
        "        lstm_cn: hidden lstm layer for Chinese\n",
        "        fc1: FNN layer 1\n",
        "        fc2: FNN layer 2\n",
        "        fc3: FNN layer 3\n",
        "        out: the output of the model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(LSTM_with_BERT, self).__init__()\n",
        "\n",
        "        self.lstm_en = nn.LSTM(768, 256, batch_first=True)\n",
        "        self.lstm_cn = nn.LSTM(768, 256, batch_first=True)\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.out = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, cn, en):\n",
        "        lstm_out_en, _ = self.lstm_en(en.view(1,-1,768))\n",
        "        lstm_out_cn, _ = self.lstm_cn(cn.view(1,-1,768))\n",
        "\n",
        "        fc_en = lstm_out_en[:,-1,:]\n",
        "        fc_cn = lstm_out_cn[:,-1,:]\n",
        "        fc_input = torch.cat((fc_en,fc_cn),1)\n",
        "\n",
        "        f1 = F.relu(self.fc1(fc_input))\n",
        "        f1 = F.relu(self.fc2(f1))\n",
        "        f1 = F.relu(self.fc3(f1))\n",
        "        f1 = F.relu(self.fc4(f1))\n",
        "        out = self.out(f1)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_zgShJOG3eT",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGtGEhGq4QYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "def check_train_performance(model):\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(english_train_bert)):\n",
        "            x = torch.tensor(english_train_bert[i])\n",
        "            y = torch.tensor(chinese_train_bert[i])\n",
        "            z = torch.tensor(score_train[i])\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=dtype)\n",
        "            z = z.to(device=device, dtype=torch.float)\n",
        "\n",
        "            score = model(x, y)\n",
        "            \n",
        "\n",
        "            predictions.append(score.cpu().detach().numpy())\n",
        "\n",
        "    predictions = np.asarray([i for item in predictions for i in item]).squeeze(1)\n",
        "    \n",
        "    pearson = pearsonr(score_train, predictions)\n",
        "    print(f'RMSE: {rmse(predictions, score_train)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "def train_part(model, optimizer, scheduler=None, epochs=1):\n",
        "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        #for t, (x, y) in enumerate(loader_train):\n",
        "        for i in range(len(english_train_bert)):\n",
        "            x = torch.tensor(english_train_bert[i])\n",
        "            y = torch.tensor(chinese_train_bert[i])\n",
        "            z = torch.tensor(score_train[i])\n",
        "            model.train()  # put model to training mode\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=dtype)\n",
        "            z = z.to(device=device, dtype=torch.float)\n",
        "\n",
        "\n",
        "            scores = model(x, y)\n",
        "\n",
        "            scores = scores.squeeze(1)\n",
        "\n",
        "            loss = F.mse_loss(scores, z)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            #if i % print_every == 0:\n",
        "               # print('Epoch: %d, Iteration %d, loss = %.4f' % (e, i, loss.item()))\n",
        "                #check_accuracy(loader_val, model)\n",
        "               # print()\n",
        "        check_train_performance(model)\n",
        "        # Adjust the learning rate\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxHS-QYz4o4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_bert_model = LSTM_with_BERT()\n",
        "print(lstm_bert_model)\n",
        "optimizer = optim.Adam(lstm_bert_model.parameters())\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 10)\n",
        "train_part(lstm_bert_model, optimizer, scheduler, epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_c87Dy8G6-y",
        "colab_type": "text"
      },
      "source": [
        "### Validation Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VpNuURs7_KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_performance_check(model):\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(english_val_bert)):\n",
        "            x = torch.tensor(english_val_bert[i])\n",
        "            y = torch.tensor(chinese_val_bert[i])\n",
        "            z = torch.tensor(score_validation[i])\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=dtype)\n",
        "            z = z.to(device=device, dtype=torch.float)\n",
        "\n",
        "            score = model(x, y)\n",
        "            \n",
        "\n",
        "            predictions.append(score.cpu().detach().numpy())\n",
        "\n",
        "    predictions = np.asarray([i for item in predictions for i in item]).squeeze(1)\n",
        "    \n",
        "    pearson = pearsonr(score_validation, predictions)\n",
        "    print(f'RMSE: {rmse(predictions, score_validation)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "val_performance_check(lstm_bert_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}