{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVR+Sentence_embed_sentence_transformer+raw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWVNrSepufI",
        "colab_type": "text"
      },
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m-5YiHHftBX",
        "colab_type": "text"
      },
      "source": [
        "## Dowdload and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNdY9X-fSlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A0Z6AWU37Gg",
        "colab_type": "text"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w8HNiip39OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path):\n",
        "    \"\"\"\n",
        "    Read data from the data path.\n",
        "\n",
        "    Args: \n",
        "        path: the path of the dataset, normally in local folder.\n",
        "    \n",
        "    Returns:\n",
        "        Loaded raw dataset. \n",
        "    \"\"\"\n",
        "    with open(path) as dataset:\n",
        "        raw_data = dataset.readlines()\n",
        "\n",
        "    return raw_data\n",
        "\n",
        "\n",
        "# Define the path of the train dataset\n",
        "english_train_path = \"train.enzh.src\"\n",
        "chinese_train_path = \"train.enzh.mt\"   \n",
        "scores_train_path = \"train.enzh.scores\"\n",
        "# Define the path of the validatin dataset\n",
        "english_validation_path = \"dev.enzh.src\"\n",
        "chinese_validation_path = \"dev.enzh.mt\"   \n",
        "scores_validation_path = \"dev.enzh.scores\"\n",
        "# Define the path of the test dataset\n",
        "english_test_path = \"test.enzh.src\"\n",
        "chinese_test_path = \"test.enzh.mt\"\n",
        "\n",
        "\n",
        "# Read train, validation, test data\n",
        "raw_english_train = read_data(english_train_path)\n",
        "raw_chinese_train = read_data(chinese_train_path)\n",
        "raw_english_validation = read_data(english_validation_path)\n",
        "raw_chinese_validation = read_data(chinese_validation_path)\n",
        "raw_english_test = read_data(english_test_path)\n",
        "raw_chinese_test = read_data(chinese_test_path)\n",
        "\n",
        "# read scores for train and validation dataset \n",
        "score_train = read_data(scores_train_path)\n",
        "score_validation = read_data(scores_validation_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej1vYxsTFdH3",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Embedding - SentenceTransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z99k58qJmwV",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rGZoOfqXDQ5",
        "colab_type": "code",
        "outputId": "2339359d-b978-4de2-d991-6739b7a9c956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/91/c85ddef872d5bb39949386930c1f834ac382e145fcd30155b09d6fb65c5a/sentence-transformers-0.2.5.tar.gz (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.3MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 17.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.11.15)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.14.15)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5-cp36-none-any.whl size=64942 sha256=991b386ef01dc35c24dea3be78f8f553f80c5bb9f17a04c3e50f511deb07fe36\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/ce/39/5bbda8ac34eb52df8c6531382ca077773fbfcbfb6386e5d66c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=ef62b01d3bac52938e758566cf12bba43c9c895af4cfa1adb43fea19c96625c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7vteCDNet8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "8b3c22d9-f233-41d5-db15-1b0083037c9f"
      },
      "source": [
        "# Import sentence transformer for sentence embedding.\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 504M/504M [02:42<00:00, 3.11MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-UqnllRiC-6",
        "colab_type": "text"
      },
      "source": [
        "Using raw corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE0Ijz9Oy80o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert preprocessed corpus to sentence embeddings\n",
        "english_train_embeddings = model.encode(raw_english_train)\n",
        "chninese_train_embeddings = model.encode(raw_chinese_train)\n",
        "\n",
        "english_val_embeddings = model.encode(raw_english_validation)\n",
        "chinese_val_embeddings = model.encode(raw_chinese_validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z0rLZSUZLjA",
        "colab_type": "text"
      },
      "source": [
        "#### Concatenate vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLS7vjqXd_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate vectors\n",
        "sentence_embeddings_train = []\n",
        "sentence_embeddings_val = []    \n",
        "sentence_embeddings_test = []\n",
        "\n",
        "# Concatenate train vectors\n",
        "for i in range(len(english_train_embeddings)):\n",
        "    english = list(english_train_embeddings[i])\n",
        "    chinese = list(chninese_train_embeddings[i])\n",
        "    english.extend(chinese)\n",
        "    sentence_embeddings_train.append(english)\n",
        "\n",
        "# Concatenate validation vectors\n",
        "for i in range(len(english_val_embeddings)):\n",
        "    english = list(english_val_embeddings[i])\n",
        "    chinese = list(chinese_val_embeddings[i])\n",
        "    english.extend(chinese)\n",
        "    sentence_embeddings_val.append(english)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRe8WNXyYoDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_embeddings_train = np.asarray(sentence_embeddings_train).astype(float)\n",
        "sentence_embeddings_val = np.asarray(sentence_embeddings_val).astype(float)\n",
        "sentence_embeddings_test = np.asarray(sentence_embeddings_test).astype(float)\n",
        "\n",
        "score_train = np.asarray(score_train).astype(float)\n",
        "score_validation = np.asarray(score_validation).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxB7hNT7Fh5X",
        "colab_type": "text"
      },
      "source": [
        "## Model: SVR\n",
        "\n",
        "It achieves the best result when k is set as 'rbf' based on experiment (in the commented part)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8uqnvCJoimG",
        "colab_type": "code",
        "outputId": "34e5d423-9328-4465-8944-60a0cea8e0ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    \"\"\"\n",
        "    Method to calculate the root mean squared error.\n",
        "\n",
        "    Args: \n",
        "        predictions: the prediction of the model.\n",
        "        targets: the ground truth.\n",
        "    \n",
        "    Returns:\n",
        "        The sentence vector.\n",
        "    \"\"\"\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "# Train and fit into the model.\n",
        "clf_t = SVR(kernel='rbf')\n",
        "clf_t.fit(sentence_embeddings_train, score_train)\n",
        "predictions = clf_t.predict(sentence_embeddings_val)\n",
        "pearson = pearsonr(score_validation, predictions)\n",
        "print(f'RMSE: {rmse(predictions,score_validation)} Pearson {pearson[0]}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.875252344881576 Pearson 0.34235343131352036\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}